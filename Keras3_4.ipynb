{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras3.4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNoHpYUrNCeGsAZMN9Chfp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moung1012/Numpy-Pandas/blob/master/Keras3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5hrHyVDHPfF"
      },
      "source": [
        "# 영화 리뷰 분류 이진 분류 예제\n",
        "- 인터넷 영화 데이터베이스로부터 가져온 양극단의 리뷰 5만개로 이루어진 데이터셋\n",
        "- 훈련 데이터 2만 5000개와 테스트 데이터 2만 5000개로 나뉘어 있고 50%는 부정 50%는 긍정 리뷰로 구성되어 있다\n",
        "- 같은 데이터에서 훈련하고 테스트 해서는 절대 안된다\n",
        "- 중요한 것은 새로운 데이터에 대한 모델의 성능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws-A8QVCIBJ8"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data,test_labels) = imdb.load_data(num_words=10000)\n",
        "# nom_words = 10000 매개 변수는 훈련 데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미\n",
        "# train_data와 test_data는 리뷰의 목록\n",
        "# 각 리뷰의 단어 인덱스의 리스트(단어 시퀀스가 인코딩 된것)\n",
        "#train_labels와 test_labels는 부정을 나타내는 0과 긍정을 나타내는 1의 리스트"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP1N9pvhIXGt"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxCAHXSQI_-A"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVonKbhTJDYJ"
      },
      "source": [
        "max([max(sequence)] for sequence in train_data) # 단어 1만개로 제한했기 때문에 인덱스는 9,999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z13XgwvIJQYJ"
      },
      "source": [
        "word_index = imdb.get_word_index() # word_index는 단어와 정수 인덱스를 매핑한 딕셔너리\n",
        "revers_word_index= dict(\n",
        "    [(value, key) for (key, value) in word_index.items()]) # 정수 인덱스와 단어를 매핑하도록 뒤집는다\n",
        "decoded_review = ' '.join(\n",
        "    [reverse_word_index.get(i - 3,'?')for i in train_data[0]]) # 리뷰를 디코딩한다 0,1,2는 '패딩','문서 시작','사전에 없음'을 위한 인덱스이므로 3을 뺀다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-ONywwKqzO"
      },
      "source": [
        "### 데이터 준비  \n",
        "리스트를 텐서로 바꾸는 두가지 방법  \n",
        "- 같은 길이가 되도록 리스트에 패딩을 추가하고 (samples, sequence_length)크기의 정수 텐서로 변환 그 다음 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용  \n",
        "- 리시트를 원-핫 인코딩(one-hot incoding)하여 0과 1의 벡터로 변환한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7VHexGsMGAp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # 크기가 (len(sequences), dimension))이고 모든 원소가 0인 행렬을 만듭니다\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # results[i]에서 특정 인덱스의 위치를 1로 만듭니다\n",
        "    return results\n",
        "\n",
        "# 훈련 데이터를 벡터로 변환합니다\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# 테스트 데이터를 벡터로 변환합니다\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6J3dqt6MhSI"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3fUEGppM08o"
      },
      "source": [
        "# 레이블을 벡터로 바꿉니다\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkco3uShM9K1"
      },
      "source": [
        "### 신경망 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4a-CYhNFMS"
      },
      "source": [
        "Dense 층을 쌓을 때 두가지 중요한 구조상의 결정이 필요하다  \n",
        "- 얼마나 많은 층을 사용할 것인가\n",
        "- 각 층에 얼마나 많은 은닉 유닛을 둘 것인가  \n",
        "현재 모델의 구조  \n",
        "- 16개의 은닉 유닛을 가진 2개의 은닉 층\n",
        "- 현재 리뷰의 감정을 스칼라 값의 예측으로 출력하는 세 번째 층"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVmVWkPdNmd"
      },
      "source": [
        "다음이 이 신경망의 모습입니다:\n",
        "\n",
        "![3-layer network](https://s3.amazonaws.com/book.keras.io/img/ch3/3_layer_network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNfI1aHldyHY"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "# 중간의 은닉층은 활성화 함수 relu를 사용하고 마지막 층은 확률을 출력하기 위해 시그모이드 활성화 함수 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnmaNxDYeCE5"
      },
      "source": [
        "---\n",
        "마지막으로 손실 함수와 옵티마이저를 선택  \n",
        "- 이진 분류 문제이고 신경망의 출력이 확률이기 때문에 binary_crossentropy 손실이 적합\n",
        "- 확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택\n",
        "- 크로스엔트로피는 정보 이론 뷴야에서 온 개념으로 확률 분포 간의 차이를 측정한다(여기서는 원본 분포와 예측 분포 사이를 측정)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K1EWqzZe0i_"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#rmsprop 옵티마이저와 binary_crossentropy 손실 함수로 모델을 설정하는 단계"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvrLEuxee21W"
      },
      "source": [
        "rmsprop, binary_crossentropy, accuracy가 포함되어 있기 때문에 옵티마이저, 손실 함수, 측정 지표를 문자열로 지정하는 것이 가능  \n",
        "- 옵티마이저의 매개변수를 바꾸거나 자신만의 손실 함수, 측정 함수를 전달해야 할 경우 옵티마이저 파이썬 클래스를 사용해 직접 만들어 옵티마이저 매개변수에 전달"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5uqXyDIfOQd"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qIkjDvPfQFp"
      },
      "source": [
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNOrDnithgWO"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7sAwWHLhilK"
      },
      "source": [
        "### 훈련 검증\n",
        "- 훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해서는 훈련 데이터에서 10,000의 샘프을 떼어서 검증 세트를 만들어야 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bckKNxEyhrtO"
      },
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-6b0Q3hs73"
      },
      "source": [
        "- 모델 512개 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련 시킨다  \n",
        "- 동시에 따로 떼어 놓은 만 개의 샘플에서 손실과 정확도를 측정해 보자\n",
        "- 이렇게 하기 위해선 validation_data 매개변수에 검증 데이터를 전달 해야한다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYYF_efsiGim"
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmWnViEIiHIN"
      },
      "source": [
        "- model.fit() 메서드는 History 객체를 반환한다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz-xVTPNiW3K"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "#히스토리 객체는 훈련하는 동안 발생한 모든 정보를 담고있는 딕셔너리인 히스토리 속성을 가지고 있다\n",
        "#이 딕셔너리는 훈련과 검증하는 동안 모니터링할 측정 지표당 하나씩 모두 내게의 항목을 담고 있다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPUOTk6hiXtg"
      },
      "source": [
        "맷플롯립을 사용해 훈련과 검증 데이터에 대한 손실과 정확도를 그려보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUOKbtmEisjZ"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX_y2o69iuGi"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# ‘bo’는 파란색 점을 의미합니다\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# ‘b’는 파란색 실선을 의미합니다\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl6zqbwXiu9R"
      },
      "source": [
        "plt.clf()   # 그래프를 초기화합니다\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LekYG3QKjAX1"
      },
      "source": [
        "- 점선은 훈련 손실과 정확도\n",
        "- 실선은 검증 손실과 정확도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XIeXbZKjXpv"
      },
      "source": [
        "- 훈련 손실이 에포크마다 감소하고 훈련 정확도는 에포크마다 증가한다  \n",
        "- 2번째 에포크 이후부터 훈련 데이터에 과도하게 최적화 되어 훈련 데이터에 특화된 표현을 학습하므로 훈련 세트 이외의 데이터에서는 일반화 하지 못한다\n",
        "- 이런 경우 과대 적합을 방지하기 위해서 3번째 에포크 이후에 훈련을 중지할 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpZClYtQkkNd"
      },
      "source": [
        "#새로운 신경망을 4번의 에포크 동안만 훈련하고 테스트 데이터에서 평가해 보자\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d08TuvukoMt"
      },
      "source": [
        "results #단순한 방식으로도 87%의 정확도를 달성했다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rYnRe4dk1I2"
      },
      "source": [
        "### 훈련된 모델로 새로운 데이터에 대해 예측하기\n",
        "- predict 메서드를 사용해서 어떤 리뷰가 긍정일 확률을 예측할 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaOP6NcglW6b"
      },
      "source": [
        "model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9H2seKplfTK"
      },
      "source": [
        "모델은 어떤 샘플에 대해 확신을 가지고 있지만 어떤 샘플에 대해서는 확신이 부족하다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyUoQjEGltep"
      },
      "source": [
        "### 추가 실험\n",
        "- 은닉층 1개 또는 3개의 은닉 층을 사용하고 검증과 테스트 정확도에 어떤 영향을 미치는지 확인해보기\n",
        "- 층의 은닉 유닛을 추가하거나 줄여보자 32개 또는 64개\n",
        "- mse손실 함수 사용해보기\n",
        "- relu 대신에 tanh 활성화 함수 사용해 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy4m9d-1mI_3"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='tanh', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC7PQ9mYmbvg"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yoTL0YkmxBW"
      },
      "source": [
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIn7voA9mz-d"
      },
      "source": [
        "results #손실 함수가 엄청나다 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTa2vBnam40E"
      },
      "source": [
        "### 정리\n",
        "- 원본 데이터를 신경망에 텐서로 주입하기 위해서는 꽤 많은 전처리가 필요하다\n",
        "- relu 활성화 함수와 함께 Dense층을 쌓은 네트워크는 앞으로 자주 사용하게 될 것이다\n",
        "- 이진 분류 문제에서 네트워크는 하나의 유닛과 sigmoid 활성화 함수를 가진 Dense층으로 끝나야 한다. 이 신경망의 출력은 확률을 나타내는 0과 1사이의 스칼라 값\n",
        "- rmsporp 옵티마이저는 문제에 상관없이 일반적으로 충분히 좋은 선택이다\n",
        "- 훈련 데이터에 대해 성능이 향상됨에 따라 신경망은 과대적합되기 시작하고 이전에 본적 없는 데이터에서는 결과가 점점 나빠지게 된다. 항상 훈련 세트 이외에 데이터에서 성능을 모니터링 해야한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Loar0qoXNV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}